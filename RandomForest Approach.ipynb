{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = 'training_data.csv'\n",
    "tournament_data = 'tournament_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "1. Extracting features and targets from the data\n",
    "2. Converting 'era' column to integer to input it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in list(train) if \"feature\" in f]\n",
    "targets = ['target_bernie']\n",
    "features.insert(0, 'era')\n",
    "\n",
    "X = train[features]\n",
    "Y = train[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X['era'] = X['era'].apply(lambda x: re.sub('[era]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle & Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x Shape ::  (336830, 51)\n",
      "Train_y Shape ::  (336830, 1)\n",
      "Test_x Shape ::  (165902, 51)\n",
      "Test_y Shape ::  (165902, 1)\n"
     ]
    }
   ],
   "source": [
    "X, Y = shuffle(X, Y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "print \"Train_x Shape :: \", X_train.shape\n",
    "print \"Train_y Shape :: \", y_train.shape\n",
    "print \"Test_x Shape :: \", X_test.shape\n",
    "print \"Test_y Shape :: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_jobs=-1)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "1. Using GridSearchCV for hyper parameter optimization\n",
    "2. RandomForestClassifier for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [20, 25], 'max_features': [2], 'random_state': [0], 'min_samples_leaf': [150, 200, 250]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "        'n_estimators': [ 20,25 ],\n",
    "        'random_state': [ 0 ],\n",
    "        'max_features': [ 2 ],\n",
    "        'min_samples_leaf': [150,200,250]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "clf = GridSearchCV(estimator=model, param_grid=parameters)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.6080901344892082\n",
      "Test Accuracy  : 0.5145929524659136\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print \"Train Accuracy :\", accuracy_score(y_train, clf.predict(X_train))\n",
    "print \"Test Accuracy  :\", accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(tournament_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['era'] = test['era'].apply(lambda x: re.sub('[era]', '', x))\n",
    "# test['era'] = test['era'].apply(lambda x: re.sub('[X]', '187', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.data_type == 'validation']\n",
    "x_prediction = test[features]\n",
    "y_prediction = clf.predict_proba(x_prediction)\n",
    "results = y_prediction[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined:                  id  probability\n",
      "0  n00183d2eed2c463     0.530454\n",
      "1  n00187fa497c9d5b     0.526056\n",
      "2  n001acd04f3617b7     0.473081\n",
      "3  n0043a13252683ee     0.515409\n",
      "4  n00535e274720abd     0.512697\n"
     ]
    }
   ],
   "source": [
    "ids = test['id']\n",
    "\n",
    "# Create your submission\n",
    "results_df = pd.DataFrame(data={'probability': results})\n",
    "joined = pd.DataFrame(ids).join(results_df)\n",
    "print \"joined:\", joined.head()\n",
    "\n",
    "# Save the predictions out to a CSV file.\n",
    "joined.to_csv(\"bernie_submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"bernie_submission2.csv\")\n",
    "\n",
    "# v = test[ test.data_type == 'validation' ].copy()\n",
    "validate_set = test.copy()\n",
    "\n",
    "validate_set = validate_set.merge( s, on = 'id', how = 'left' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'era', u'data_type', u'feature1', u'feature2', u'feature3',\n",
       "       u'feature4', u'feature5', u'feature6', u'feature7', u'feature8',\n",
       "       u'feature9', u'feature10', u'feature11', u'feature12', u'feature13',\n",
       "       u'feature14', u'feature15', u'feature16', u'feature17', u'feature18',\n",
       "       u'feature19', u'feature20', u'feature21', u'feature22', u'feature23',\n",
       "       u'feature24', u'feature25', u'feature26', u'feature27', u'feature28',\n",
       "       u'feature29', u'feature30', u'feature31', u'feature32', u'feature33',\n",
       "       u'feature34', u'feature35', u'feature36', u'feature37', u'feature38',\n",
       "       u'feature39', u'feature40', u'feature41', u'feature42', u'feature43',\n",
       "       u'feature44', u'feature45', u'feature46', u'feature47', u'feature48',\n",
       "       u'feature49', u'feature50', u'target_bernie', u'target_elizabeth',\n",
       "       u'target_jordan', u'target_ken', u'target_charles', u'target_frank',\n",
       "       u'target_hillary', u'probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test.data_type.value_counts()\n",
    "v.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Logloss & Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "0.6923081173738794\n",
      "121 4540 69.23% True\n",
      "0.6912390382114364\n",
      "122 4626 69.12% True\n",
      "0.6932699624682883\n",
      "123 4582 69.33% False\n",
      "0.6917412360787164\n",
      "124 4604 69.17% True\n",
      "0.6933232899066735\n",
      "125 4673 69.33% False\n",
      "0.6926990963416928\n",
      "126 4663 69.27% True\n",
      "0.6941678699649289\n",
      "127 4675 69.42% False\n",
      "0.6931102180697901\n",
      "128 4632 69.31% True\n",
      "0.6909974837907648\n",
      "129 4713 69.10% True\n",
      "0.6917604609580587\n",
      "130 4752 69.18% True\n",
      "0.6926105869193974\n",
      "131 4817 69.26% True\n",
      "0.694854072438013\n",
      "132 4807 69.49% False\n",
      "\n",
      "consistency: 66.7% (8/12)\n",
      "log loss:    69.27%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from math import log\n",
    "\n",
    "eras = validate_set.era.unique()\n",
    "print len(eras)\n",
    "\n",
    "good_eras = 0\n",
    "\n",
    "for era in eras:\n",
    "    tmp = validate_set[ validate_set.era == era ]\n",
    "    logloss = metrics.log_loss( tmp.target_bernie, tmp.probability )\n",
    "    print logloss\n",
    "    is_good = logloss < -log( 0.5 )\n",
    "\n",
    "    if is_good:\n",
    "        good_eras += 1\n",
    "\n",
    "    print( \"{} {} {:.2%} {}\".format( era, len( tmp ), logloss, is_good ))\n",
    "\n",
    "consistency = good_eras / float( len( eras ))\n",
    "print( \"\\nconsistency: {:.1%} ({}/{})\".format( consistency, good_eras, len( eras )))\n",
    "\n",
    "logloss = metrics.log_loss( validate_set.target_bernie, validate_set.probability )\n",
    "print( \"log loss:    {:.2%}\\n\".format( logloss ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
